{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation coefficent vs accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05\n",
      "(192, 5272)\n",
      "(192, 5074)\n",
      "(192, 2480)\n",
      "(192, 6)\n",
      "Fitting 10 folds for each of 162 candidates, totalling 1620 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 1620 out of 1620 | elapsed:   17.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.91748621645\n",
      "0.1\n",
      "(192, 5272)\n",
      "(192, 5074)\n",
      "(192, 2480)\n",
      "(192, 8)\n",
      "Fitting 10 folds for each of 162 candidates, totalling 1620 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 1620 out of 1620 | elapsed:   23.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.4044571948\n",
      "0.15\n",
      "(192, 5272)\n",
      "(192, 5074)\n",
      "(192, 2480)\n",
      "(192, 11)\n",
      "Fitting 10 folds for each of 162 candidates, totalling 1620 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 1620 out of 1620 | elapsed:   27.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.5568760093\n",
      "0.2\n",
      "(192, 5272)\n",
      "(192, 5074)\n",
      "(192, 2480)\n",
      "(192, 19)\n",
      "Fitting 10 folds for each of 162 candidates, totalling 1620 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 1620 out of 1620 | elapsed:   18.8s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.38123562429\n",
      "0.25\n",
      "(192, 5272)\n",
      "(192, 5074)\n",
      "(192, 2480)\n",
      "(192, 27)\n",
      "Fitting 10 folds for each of 162 candidates, totalling 1620 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 1620 out of 1620 | elapsed:   22.8s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.76304057272\n",
      "0.3\n",
      "(192, 5272)\n",
      "(192, 5074)\n",
      "(192, 2480)\n",
      "(192, 34)\n",
      "Fitting 10 folds for each of 162 candidates, totalling 1620 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 1620 out of 1620 | elapsed:   27.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.27704943638\n",
      "0.35\n",
      "(192, 5272)\n",
      "(192, 5074)\n",
      "(192, 2480)\n",
      "(192, 42)\n",
      "Fitting 10 folds for each of 162 candidates, totalling 1620 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 1620 out of 1620 | elapsed:   40.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.57362233803\n",
      "0.4\n",
      "(192, 5272)\n",
      "(192, 5074)\n",
      "(192, 2480)\n",
      "(192, 49)\n",
      "Fitting 10 folds for each of 162 candidates, totalling 1620 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 1620 out of 1620 | elapsed:   42.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.95318532848\n",
      "0.45\n",
      "(192, 5272)\n",
      "(192, 5074)\n",
      "(192, 2480)\n",
      "(192, 57)\n",
      "Fitting 10 folds for each of 162 candidates, totalling 1620 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 1620 out of 1620 | elapsed:   41.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.79296299764\n",
      "0.5\n",
      "(192, 5272)\n",
      "(192, 5074)\n",
      "(192, 2480)\n",
      "(192, 80)\n",
      "Fitting 10 folds for each of 162 candidates, totalling 1620 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 1620 out of 1620 | elapsed:   37.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.08389814065\n",
      "0.55\n",
      "(192, 5272)\n",
      "(192, 5074)\n",
      "(192, 2480)\n",
      "(192, 95)\n",
      "Fitting 10 folds for each of 162 candidates, totalling 1620 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 1620 out of 1620 | elapsed:   34.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6986531905\n",
      "0.6\n",
      "(192, 5272)\n",
      "(192, 5074)\n",
      "(192, 2480)\n",
      "(192, 120)\n",
      "Fitting 10 folds for each of 162 candidates, totalling 1620 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 1620 out of 1620 | elapsed:   34.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.12596927857\n",
      "0.65\n",
      "(192, 5272)\n",
      "(192, 5074)\n",
      "(192, 2480)\n",
      "(192, 147)\n",
      "Fitting 10 folds for each of 162 candidates, totalling 1620 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 1620 out of 1620 | elapsed:   33.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.33551626878\n",
      "0.7\n",
      "(192, 5272)\n",
      "(192, 5074)\n",
      "(192, 2480)\n",
      "(192, 172)\n",
      "Fitting 10 folds for each of 162 candidates, totalling 1620 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 1620 out of 1620 | elapsed:   34.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.04074155515\n",
      "0.75\n",
      "(192, 5272)\n",
      "(192, 5074)\n",
      "(192, 2480)\n",
      "(192, 225)\n",
      "Fitting 10 folds for each of 162 candidates, totalling 1620 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 1620 out of 1620 | elapsed:   34.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.054211628\n",
      "0.8\n",
      "(192, 5272)\n",
      "(192, 5074)\n",
      "(192, 2480)\n",
      "(192, 287)\n",
      "Fitting 10 folds for each of 162 candidates, totalling 1620 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 1620 out of 1620 | elapsed:   38.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.25094176907\n",
      "0.85\n",
      "(192, 5272)\n",
      "(192, 5074)\n",
      "(192, 2480)\n",
      "(192, 371)\n",
      "Fitting 10 folds for each of 162 candidates, totalling 1620 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 1620 out of 1620 | elapsed:   42.8s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.85868238768\n",
      "0.9\n",
      "(192, 5272)\n",
      "(192, 5074)\n",
      "(192, 2480)\n",
      "(192, 494)\n",
      "Fitting 10 folds for each of 162 candidates, totalling 1620 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 1620 out of 1620 | elapsed:   50.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.72164008\n",
      "0.95\n",
      "(192, 5272)\n",
      "(192, 5074)\n",
      "(192, 2480)\n",
      "(192, 740)\n",
      "Fitting 10 folds for each of 162 candidates, totalling 1620 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 1620 out of 1620 | elapsed:  1.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.63117231356\n",
      "1.0\n",
      "(192, 5272)\n",
      "(192, 5074)\n",
      "(192, 2480)\n",
      "(192, 1996)\n",
      "Fitting 10 folds for each of 162 candidates, totalling 1620 fits\n",
      "1.81675493614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 1620 out of 1620 | elapsed:  2.6min finished\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "pd.options.mode.chained_assignment = None  \n",
    "from sklearn.svm import SVR\n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "#random state for division of dataset\n",
    "#Vary this to 3,4,5,6,7 to reproduce results in the paper\n",
    "randomstate = 3 \n",
    "\n",
    "# Drop columns with correlation above defined threshold\n",
    "def drop_correlated_cols(df, cols_features, threshold):\n",
    "    cols_to_remove = set()\n",
    "    correlations_abs = df[cols_features].corr().abs()\n",
    "    for i in range(len(correlations_abs.columns)):\n",
    "        for j in range(i):\n",
    "            if (correlations_abs.iloc[i, j] >= threshold) and (correlations_abs.columns[j] not in cols_to_remove):\n",
    "                colname = correlations_abs.columns[i]\n",
    "                if colname in df.columns:\n",
    "                    cols_to_remove.add(colname)\n",
    "                    df.drop(colname, axis=1, inplace=True)\n",
    "    return df\n",
    "\n",
    "for i in range(0,100,5):\n",
    "    \n",
    "    print(i/100)\n",
    "    \n",
    "    # Load the entire set\n",
    "    df = pd.read_excel('Thermo_data_cyclic_species.xlsx', na_values=['na', 'nan'], index_col=0)\n",
    "    print(df.shape)\n",
    "\n",
    "    # Drop columns with na\n",
    "    df.dropna(axis='columns', inplace=True)\n",
    "    print(df.shape)\n",
    "\n",
    "    # Drop columns that have only a single value (variance = 0)\n",
    "    count_unique = df.apply(pd.Series.nunique)\n",
    "    cols_to_drop = count_unique[count_unique == 1].index\n",
    "    df.drop(columns=cols_to_drop, inplace=True)\n",
    "    print(df.shape)\n",
    "\n",
    "    # Drop correlated features\n",
    "    cols_features = df.columns[2:]\n",
    "    df = drop_correlated_cols(df, cols_features, i/100)\n",
    "    print(df.shape)\n",
    "\n",
    "    target = df['Enthalpy(KCal)']\n",
    "    features = df[df.columns[2:]]\n",
    "    \n",
    "    #split the dataset\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, target, random_state=randomstate, test_size=0.1)\n",
    "    dfpre=X_train\n",
    "    dfprenon=X_train\n",
    "    Testing=X_test\n",
    "\n",
    "    #Training Normalization\n",
    "    numerical=list(dfpre)\n",
    "    scaler = MinMaxScaler()\n",
    "    dfpre[numerical] = scaler.fit_transform(dfpre[numerical])\n",
    "    X_train=dfpre\n",
    "\n",
    "    #Testing Normalization\n",
    "    test = scaler.transform(Testing)\n",
    "    X_test=test\n",
    "\n",
    "    # Fit regression model\n",
    "    # Define search space\n",
    "    Cs = [1, 10, 100, 500, 1000, 2000, 3000, 4000, 5000, 6000, 7000, 8000, 9000, 10000, 12500, 15000, 20000, 50000]\n",
    "    epsilons = [0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5]\n",
    "\n",
    "    # Setup the grid to be searched over\n",
    "    param_grid = dict(C=Cs, epsilon=epsilons)\n",
    "\n",
    "    y_poly=GridSearchCV(SVR(kernel='rbf'), param_grid, cv=KFold(n_splits=10, shuffle =True, random_state=randomstate), n_jobs=1,verbose=1,scoring='neg_mean_squared_error')\n",
    "    y_poly.fit(X_train, y_train)\n",
    "    y_pred_SVR= y_poly.predict(X_test)\n",
    "\n",
    "\n",
    "    error_ma = mean_absolute_error(y_test, y_pred_SVR)\n",
    "\n",
    "    print(error_ma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reduce the dataset using fianl correlation coefficient = 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(192, 5272)\n",
      "(192, 5074)\n",
      "(192, 2480)\n",
      "(192, 49)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Drop columns with correlation above defined threshold\n",
    "def drop_correlated_cols(df, cols_features, threshold):\n",
    "    cols_to_remove = set()\n",
    "    correlations_abs = df[cols_features].corr().abs()\n",
    "    for i in range(len(correlations_abs.columns)):\n",
    "        for j in range(i):\n",
    "            if (correlations_abs.iloc[i, j] >= threshold) and (correlations_abs.columns[j] not in cols_to_remove):\n",
    "                colname = correlations_abs.columns[i]\n",
    "                if colname in df.columns:\n",
    "                    cols_to_remove.add(colname)\n",
    "                    df.drop(colname, axis=1, inplace=True)\n",
    "    return df\n",
    "\n",
    "# Load the entire set\n",
    "df = pd.read_excel('Thermo_data_cyclic_species.xlsx', na_values=['na', 'nan'], index_col=0)\n",
    "print(df.shape)\n",
    "\n",
    "# Drop columns with na\n",
    "df.dropna(axis='columns', inplace=True)\n",
    "print(df.shape)\n",
    "\n",
    "# Drop columns that have only a single value (variance = 0)\n",
    "count_unique = df.apply(pd.Series.nunique)\n",
    "cols_to_drop = count_unique[count_unique == 1].index\n",
    "df.drop(columns=cols_to_drop, inplace=True)\n",
    "print(df.shape)\n",
    "\n",
    "# Drop correlated features\n",
    "cols_features = df.columns[2:]\n",
    "df = drop_correlated_cols(df, cols_features, 0.4)\n",
    "print(df.shape)\n",
    "\n",
    "\n",
    "# Save dataset\n",
    "df.to_csv('Dataset_processed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Fold error estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(192, 47)\n",
      "Fitting 10 folds for each of 162 candidates, totalling 1620 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=19)]: Done  12 tasks      | elapsed:    5.2s\n",
      "[Parallel(n_jobs=19)]: Done 500 tasks      | elapsed:    9.3s\n",
      "[Parallel(n_jobs=19)]: Done 1620 out of 1620 | elapsed:   21.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 162 candidates, totalling 1620 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=19)]: Done  12 tasks      | elapsed:    6.1s\n",
      "[Parallel(n_jobs=19)]: Done 485 tasks      | elapsed:   10.3s\n",
      "[Parallel(n_jobs=19)]: Done 1620 out of 1620 | elapsed:   23.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 162 candidates, totalling 1620 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=19)]: Done  12 tasks      | elapsed:    6.2s\n",
      "[Parallel(n_jobs=19)]: Done 420 tasks      | elapsed:    9.6s\n",
      "[Parallel(n_jobs=19)]: Done 1522 tasks      | elapsed:   20.6s\n",
      "[Parallel(n_jobs=19)]: Done 1620 out of 1620 | elapsed:   22.8s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 162 candidates, totalling 1620 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=19)]: Done  12 tasks      | elapsed:    5.4s\n",
      "[Parallel(n_jobs=19)]: Done 500 tasks      | elapsed:    9.5s\n",
      "[Parallel(n_jobs=19)]: Done 1620 out of 1620 | elapsed:   20.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 162 candidates, totalling 1620 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=19)]: Done  12 tasks      | elapsed:    5.3s\n",
      "[Parallel(n_jobs=19)]: Done 500 tasks      | elapsed:    9.0s\n",
      "[Parallel(n_jobs=19)]: Done 1620 out of 1620 | elapsed:   19.8s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 162 candidates, totalling 1620 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=19)]: Done  12 tasks      | elapsed:    5.6s\n",
      "[Parallel(n_jobs=19)]: Done 500 tasks      | elapsed:    9.5s\n",
      "[Parallel(n_jobs=19)]: Done 1620 out of 1620 | elapsed:   19.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 162 candidates, totalling 1620 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=19)]: Done  12 tasks      | elapsed:    5.6s\n",
      "[Parallel(n_jobs=19)]: Done 500 tasks      | elapsed:    9.5s\n",
      "[Parallel(n_jobs=19)]: Done 1620 out of 1620 | elapsed:   21.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 162 candidates, totalling 1620 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=19)]: Done  12 tasks      | elapsed:    5.4s\n",
      "[Parallel(n_jobs=19)]: Done 500 tasks      | elapsed:    9.5s\n",
      "[Parallel(n_jobs=19)]: Done 1620 out of 1620 | elapsed:   19.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 162 candidates, totalling 1620 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=19)]: Done  12 tasks      | elapsed:    5.3s\n",
      "[Parallel(n_jobs=19)]: Done 500 tasks      | elapsed:    9.3s\n",
      "[Parallel(n_jobs=19)]: Done 1620 out of 1620 | elapsed:   20.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 162 candidates, totalling 1620 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=19)]: Done  12 tasks      | elapsed:    5.3s\n",
      "[Parallel(n_jobs=19)]: Done 500 tasks      | elapsed:    9.2s\n",
      "[Parallel(n_jobs=19)]: Done 1620 out of 1620 | elapsed:   19.9s finished\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv('Dataset_processed.csv', index_col=0)\n",
    "target = df['Enthalpy(KCal)']\n",
    "features = df[df.columns[2:]]\n",
    "print(features.shape)\n",
    "\n",
    "# Define search space\n",
    "Cs = [1, 10, 100, 500, 1000, 2000, 3000, 4000, 5000, 6000, 7000, 8000, 9000, 10000, 12500, 15000, 20000, 50000]\n",
    "epsilons = [0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5]\n",
    "\n",
    "# Setup the grid to be searched over\n",
    "param_grid = dict(C=Cs, epsilon=epsilons)\n",
    "\n",
    "# Define outer folds\n",
    "kFolds = KFold(n_splits=10, shuffle=True, random_state=1).split(X=features.values, y=target.values)\n",
    "\n",
    "# Define inner folds\n",
    "grid_search = GridSearchCV(SVR(kernel='rbf', gamma = 'auto'), param_grid, cv=KFold(n_splits=10, shuffle=True, random_state=1),\n",
    "                           n_jobs=19, verbose=1, scoring='neg_mean_squared_error')\n",
    "\n",
    "# Open results file and write out headers\n",
    "out_file = open(\"grid_search_svr.csv\", 'w')\n",
    "wr = csv.writer(out_file, dialect='excel')\n",
    "headers = ['C', 'epsilon', \"r2\", \"error_ma\", \"error_ms\", \"error_rms\", \"error_mp\", \"error_max\"]\n",
    "wr.writerow(headers)\n",
    "out_file.flush()\n",
    "\n",
    "for index_train, index_test in kFolds:\n",
    "    # Get train and test splits\n",
    "    x_train, x_test = features.iloc[index_train].values, features.iloc[index_test].values\n",
    "    y_train, y_test = target.iloc[index_train].values, target.iloc[index_test].values\n",
    "\n",
    "    # Apply min max normalization\n",
    "    scaler = MinMaxScaler().fit(x_train)\n",
    "    x_train = scaler.transform(x_train)\n",
    "    x_test = scaler.transform(x_test)\n",
    "\n",
    "    # Fit\n",
    "    grid_search.fit(x_train, y_train)\n",
    "\n",
    "    # Get best params\n",
    "    best_params = grid_search.best_params_\n",
    "\n",
    "    # Calculate error metrics\n",
    "    predictions = grid_search.predict(x_test)\n",
    "    diff = y_test - predictions\n",
    "    r2 = r2_score(y_test, predictions)\n",
    "    error_ma = mean_absolute_error(y_test, predictions)\n",
    "    error_ms = mean_squared_error(y_test, predictions)\n",
    "    error_rms = np.sqrt(np.mean(np.square(diff)))\n",
    "    error_mp = np.mean(abs(np.divide(diff, y_test))) * 100\n",
    "    error_max = np.amax(np.absolute(diff))\n",
    "\n",
    "    # Write results\n",
    "    row = [best_params['C'], best_params['epsilon'], r2,\n",
    "           error_ma, error_ms, error_rms, error_mp, error_max]\n",
    "    wr.writerow(row)\n",
    "    out_file.flush()\n",
    "\n",
    "out_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Model genaration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 162 candidates, totalling 1620 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=19)]: Done  12 tasks      | elapsed:    5.9s\n",
      "[Parallel(n_jobs=19)]: Done 371 tasks      | elapsed:    9.3s\n",
      "[Parallel(n_jobs=19)]: Done 1374 tasks      | elapsed:   20.7s\n",
      "[Parallel(n_jobs=19)]: Done 1620 out of 1620 | elapsed:   26.8s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 2000, 'epsilon': 0.3}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.svm import SVR\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "# Load the entire set\n",
    "df = pd.read_csv('Dataset_processed.csv', index_col=0)\n",
    "target = df['Enthalpy(KCal)']\n",
    "features = df[df.columns[2:]]\n",
    "\n",
    "# Define search space\n",
    "Cs = [1, 10, 100, 500, 1000, 2000, 3000, 4000, 5000, 6000, 7000, 8000, 9000, 10000, 12500, 15000, 20000, 50000]\n",
    "epsilons = [0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5]\n",
    "\n",
    "# Setup the grid to be searched over\n",
    "param_grid = dict(C=Cs, epsilon=epsilons)\n",
    "\n",
    "# Define grid search\n",
    "grid_search = GridSearchCV(SVR(kernel='rbf', gamma='auto'), param_grid, cv=KFold(n_splits=10, shuffle=True, random_state=2),\n",
    "                           n_jobs=19, verbose=1, scoring='neg_mean_squared_error')\n",
    "\n",
    "# Split data in to features and target\n",
    "x_train = features.values\n",
    "y_train = target.values\n",
    "\n",
    "# Apply min max normalization\n",
    "scaler = MinMaxScaler().fit(x_train)\n",
    "x_train = scaler.transform(x_train)\n",
    "\n",
    "# Find best parameters\n",
    "grid_search.fit(x_train, y_train)\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "# Retrain model with best parameters found from grid search\n",
    "best_params = grid_search.best_params_\n",
    "model = SVR(kernel='rbf', gamma='auto', C=best_params['C'], epsilon=best_params['epsilon'])\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "# save the model\n",
    "filename = 'final_SVR_model.sav'\n",
    "pickle.dump(model, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sensitivty Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import load_model\n",
    "\n",
    "\n",
    "def get_sensitivity_scores(model, features, top_n):\n",
    "    # Get just the values of features\n",
    "    x_train = features.values\n",
    "    # Apply min max normalization\n",
    "    scaler = MinMaxScaler().fit(x_train)\n",
    "    x_train = scaler.transform(x_train)\n",
    "    # Find mean and standard deviation of each feature\n",
    "    x_train_avg = np.mean(x_train, axis=0).reshape(1, -1)\n",
    "    x_train_std = np.std(x_train, axis=0).reshape(1, -1)\n",
    "    prediction_mean = model.predict(x_train_avg)\n",
    "\n",
    "    scores_max = []\n",
    "    # Iterate over each feature\n",
    "    for i in range(x_train_avg.shape[1]):\n",
    "        # Copy x_train_avg\n",
    "        x_train_i = x_train_avg.copy()\n",
    "        # Add the standard deviation of i to that column\n",
    "        x_train_i[:, i] = x_train_i[:, i] + (0.1*x_train_std[:, i])\n",
    "        result_i = model.predict(x_train_i)\n",
    "        # Take the difference and divide by standard deviation\n",
    "        diff = (result_i - prediction_mean) / (0.1*x_train_std[:, i])\n",
    "        scores_max.append(diff.flatten()[0])\n",
    "    scores_max = np.absolute(scores_max)\n",
    "    indices_top = np.argsort(scores_max)[-top_n:]\n",
    "    features_top = features.iloc[:, indices_top].columns\n",
    "    scores_top = scores_max[indices_top]\n",
    "    return features_top, scores_top\n",
    "\n",
    "\n",
    "# Initialize result sheet\n",
    "df_sensitivity = pd.DataFrame()\n",
    "n_top = 47\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv('Dataset_processed.csv', index_col=0)\n",
    "features = df[df.columns[2:]]\n",
    "\n",
    "# Load SVR model\n",
    "filename = 'final_SVR_model.sav'\n",
    "model = pickle.load(open(filename, 'rb'))\n",
    "\n",
    "features_top, scores_top = get_sensitivity_scores(model, features, n_top)\n",
    "df_sensitivity['SVR features'] = features_top\n",
    "df_sensitivity['SVR sensitivity values (abs)'] = scores_top\n",
    "\n",
    "df_sensitivity.to_csv('sensitivity_analysis.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
